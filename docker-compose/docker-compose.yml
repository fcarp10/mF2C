version: '3.4'
services:
  ####################### Proxy  ######################
  proxy:
    image: traefik:1.7.14
    restart: unless-stopped
    command: --web --docker --docker.exposedByDefault=false --loglevel=info
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/traefik.toml:/traefik.toml
      - ./traefik/cert:/ssl:ro
    ports:
      - "80:80"
      - "443:443"
  ######################################################

  ######################## CIMI  #######################
  cimi:
    image: mf2c/cimi-server:2.28
    restart: on-failure
    depends_on:
      - logicmodule1
      - dcproxy
    environment:
      - DC_HOST=dcproxy
      - DC_PORT=6472
      - EPHEMERAL_DB_BINDING_NS=com.sixsq.slipstream.db.dataclay.loader
      - PERSISTENT_DB_BINDING_NS=com.sixsq.slipstream.db.dataclay.loader
    expose:
      - "8201"
    labels:
      - "traefik.enable=true"
      - "traefik.backend=cimi"
      - "traefik.frontend.rule=PathPrefix:/api"
    volumes:
      - ringcontainer:/opt/slipstream/ring-container
      - ringcontainerexample:/opt/slipstream/ring-example
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:8201/api/cloud-entry-point"]

  rc:
    image: sixsq/ring-container:3.52
    expose:
      - "5000"
    volumes:
      - ringcontainer:/opt/slipstream/ring-container
      - ringcontainerexample:/opt/slipstream/ring-example
    command: sh

  #################### Dataclay  #######################
  dcproxy:
    image: mf2c/dataclay-proxy:2.24
    depends_on:
      - logicmodule1
      - ds1java1
    expose:
      - "6472"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD-SHELL", "health_check/wrapper_health_check.sh"]
       
  logicmodule1:
    image: mf2c/dataclay-logicmodule:2.24
    ports:
       - "1034:1034"
    environment:
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
      - LOGICMODULE_PORT_TCP=1034
      - LOGICMODULE_HOST=logicmodule1
    volumes:
      - ./prop/global.secure.properties:/usr/src/dataclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/log4j2.xml:ro
    labels:
      - "traefik.enable=true"
      - "traefik.backend=logicmodule1"
      - "traefik.frontend.rule=Headers: service-alias,logicmodule1"
      - "traefik.port=1034"
      - "traefik.protocol=h2c"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD-SHELL", "health_check/logicmodule_health_check.sh"]

  ds1java1:
    image: mf2c/dataclay-dsjava:2.24
    depends_on:
      - logicmodule1
    environment:
      - DATASERVICE_NAME=DS1
      - DATASERVICE_HOST=ds1java1
      - DATASERVICE_JAVA_PORT_TCP=2127
      - LOGICMODULE_PORT_TCP=1034
      - LOGICMODULE_HOST=logicmodule1
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/log4j2.xml:ro
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD-SHELL", "health_check/dataservice_health_check.sh"]
       
  #####################################################

  ################### Event Manager ###################
  event-manager:
    image: mf2c/cimi-server-events:1.0
    depends_on:
      - cimi
    expose:
      - 8000
    restart: unless-stopped
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "sh", "-c", "netstat -a | grep '0.0.0.0:8000'"]
  ######################################################

  ################## Service Manager ###################
  service-manager:
    image: mf2c/service-manager:1.9.0
    restart: unless-stopped
    depends_on:
      - cimi
    expose:
      - 46200
    labels:
      - "traefik.enable=true"
      - "traefik.frontend.rule=PathPrefixStrip:/sm"
      - "traefik.port=46200"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:46200/api"]
  ######################################################

  ########## Lifecycle Manager + User Manager ##########
  lm-um:
    image: mf2c/lifecycle-usermngt:1.3.9
    restart: on-failure
    depends_on:
      - cimi
      - resource-categorization
      - analytics-engine
      - landscaper
    expose:
      - "46300"
      - "46000"
    ports:
      - "46000:46000"
      - "46300:46300"
    environment:
      - WORKING_DIR_VOLUME=/tmp/mf2c/lm/compose_files
      - UM_WORKING_DIR_VOLUME=/tmp/mf2c/um/
      - LM_WORKING_DIR_VOLUME=/tmp/mf2c/lm/
      - SERVICE_CONSUMER=true
      - RESOURCE_CONTRIBUTOR=true
      - MAX_APPS=100
    volumes:
      - /tmp/mf2c/lm/compose_files:/tmp/mf2c/lm/compose_files
      - /tmp/mf2c/um:/tmp/mf2c/um
      - /tmp/mf2c/lm:/tmp/mf2c/lm
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      start_period: 60s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:46000/api/v2/lm/isumalive"]
  ######################################################

  #################### SLA Manager #####################
  slalite:
    image: mf2c/sla-management:0.8.1
    restart: on-failure
    expose:
      - "46030"
    healthcheck:
      start_period: 30s
      retries: 5
      test: ["CMD", "wget", "--spider", "http://localhost:46030"]
  ######################################################

  ############ Resource Manager + Security #############
  policies:
     image: mf2c/policies:2.0.5
     restart: on-failure
     depends_on:
       - cimi
     ports:
       - "46050"
     labels:
       - "traefik.enable=true"
       - "traefik.frontend.rule=PathPrefix:/rm"
       - "traefik.port=46050"
     environment:
       - "DEBUG=True"
       - "MF2C=True"
       - "isLeader=${isLeader}"
       - "WIFI_DEV_FLAG=${WIFI_DEV_FLAG}"
     healthcheck:
       start_period: 30s
       retries: 5
       test: ["CMD", "wget", "--spider", "http://localhost:46050"]

  discovery:
     image: mf2c/discovery:1.4
     restart: on-failure
     depends_on:
       - cimi
     network_mode: "host"
     ports:
     - "46040:46040"
     cap_add:
       - NET_ADMIN
     healthcheck:
       start_period: 30s
       retries: 5
       test: curl --fail -s http://localhost:46040/api/v1/resource-management/discovery/ || exit 1

  identification:
     image: mf2c/identification:v2
     restart: on-failure
     volumes:
       - mydata:/data
     expose:
       - 46060
     environment:
       - mF2C_User=${usr}
       - mF2C_Pass=${pwd}
       
  cau-client:
     image: mf2c/cau-client-it2:v2.1
     depends_on:
       - policies
     volumes:
       - pkidata:/pkidata
     expose:
       - 46065
     #using the cloud cau for the moment
     environment:
       - CCAU_URL=213.205.14.13:55443
       - CAU_URL=213.205.14.13:55443
     healthcheck:
       start_period: 60s
       interval: 60s
       timeout: 60s
       retries: 5
       test: ["CMD-SHELL", "echo getpubkey=1233211234567 | nc localhost 46065"]

  aclib:
     image: mf2c/aclib:v1.0
     depends_on:
       - cau-client
     volumes:
       - pkidata:/pkidata
     expose:
       - 46080
     environment:
       - ACLIB_PORT=46080
       - CAUCLIENT_PORT=46065
     healthcheck:
       start_period: 70s
       interval: 120s
       timeout: 60s
       retries: 3
       test: ["CMD-SHELL", "echo {\"sec\":\"pro\",\"comp\":\"f\",\"payload\":\"ping\",\"typ\":\"jws\"} | nc localhost 46080"]

  resource-categorization:
     image: mf2c/resource-categorization:latest-V2.0.22
     hostname: IRILD039
     restart: on-failure
     depends_on:
       - policies
     expose:
       - 46070
     environment:
       - "agentType=${agentType}"
       - "targetDeviceActuator=${targetDeviceActuator}"
       - "targetDeviceSensor=${targetDeviceSensor}"
     privileged: true
     volumes:
       - /var/run/docker.sock:/var/run/docker.sock
       - vpninfo:/vpninfo
     healthcheck:
       start_period: 60s
       retries: 5
       test: curl --fail -s http://localhost:46070/api/v1/resource-management/categorization/run/ || exit 1

  vpnclient:
    image: mf2c/vpnclient:1.1.4
    depends_on:
      - cau-client
    network_mode: "host"
    extra_hosts:
      - "vpnserver:213.205.14.13"
    environment:
      - "VPNINFO=/vpninfo/vpnclient.status"
    cap_add:
      - NET_ADMIN
    volumes:
      - pkidata:/pkidata
      - vpninfo:/vpninfo
    healthcheck:
      start_period: 120s
      interval: 30s
      test: grep -q connected /usr/share/nginx/html/api/get_vpn_ip

  resource-bootstrap:
    image: mf2c/resource-bootstrap:1.3.0
    restart: "no"
    environment:
      - CIMI_HOST=proxy
      - CIMI_PORT=80
      # mimics identification
      - MF2C_USER=${usr}
      - MF2C_PASS=${pwd}
    depends_on:
      - proxy
      - cimi

  ##### Analytics Engine + Recomender + Landscaper #####

  analytics-engine:
    image: mf2c/recommender-test:1.0
    expose:
      - "46020"
    ports:
      - "46020:46020"
    labels:
      - "traefik.enable=true"
      - "traefik.port=46020"
      - "traefik.frontend.rule=PathPrefixStrip:/analytics-engine"

#  analytics-engine:
#    image: mf2c/analytics-engine:0.5.5
#    expose:
#      - "46020"
#    ports:
#      - "46020:46020"
#    depends_on:
#      - influxdb
#    volumes:
#      - ./analytics_engine.conf:/root/analytics_engine/analytics_engine.conf
#    labels:
#      - "traefik.enable=true"
#      - "traefik.port=46020"
#      - "traefik.frontend.rule=PathPrefixStrip:/analytics-engine"
#
  landscaper:
    image: mf2c/landscaper:0.6.14
    volumes:
    - landscaper:/landscaper/data
    - ./landscaper.cfg:/landscaper/landscaper.cfg
    - ./ls_log:/landscaper/logs
    - /var/run/docker.sock:/var/run/docker.sock
    depends_on: ["cimi", "neo4j", "proxy"]
    environment:
      - OS_TENANT_NAME=
      - OS_PROJECT_NAME=
      - OS_TENANT_ID=
      - OS_USERNAME=
      - OS_PASSWORD=
      - OS_AUTH_URL=
    command: ["./start.sh", "http://neo4j:7474"]

  telem_start:
    image: mf2c/telem_start:0.1
    command: sh /download.sh
    environment:
      - HOSTNAME
    depends_on:
      - snap

  influxdb:
    image: influxdb
    expose:
      - "8086"
    ports:
      - "8086:8086"
    volumes:
      - influx:/var/lib/influxdb
    environment:
      - INFLUXDB_DB=snap
      - INFLUXDB_USER=snap
      - INFLUXDB_USER_PASSWORD=snap
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_USER_PASSWORD=admin
      - INFLUXDB_HTTP_LOG_ENABLED=false
  snap:
    image: intelsdi/snap:xenial
    hostname: snap
    ports:
      - "8181:8181"
    volumes: ['/proc:/proc_host', '/sys/fs/cgroup:/sys/fs/cgroup', '/var/run/docker.sock:/var/run/docker.sock']
    depends_on:
      - influxdb
    environment:
      - SNAP_VERSION=2.0.0
      - SNAP_LOG_LEVEL=2

  neo4j:
    image: neo4j:2.3.12
    privileged: true
    volumes:
      - neo4j:/data
    ports:
      - "7475:7474" # expose the port for the console ui
    environment:
      - NEO4J_AUTH=neo4j/password # neo4j requires change from default password, should reflect landscape.cfg

  web:
    image: mf2c/landscaper:0.6.14
    environment:
    - PYTHONPATH=/landscaper
    volumes:
      - ./landscaper.cfg:/landscaper/landscaper.cfg
      - ./ls_log:/collector_log
    ports:
      - "9001:9001"
    depends_on:
      - neo4j
      - landscaper
    command: ["/usr/local/bin/gunicorn", "-b", "0.0.0.0:9001", "-w", "2", "--chdir", "/landscaper/landscaper/web", "application:APP"]


######################################################

volumes:
  mydata: {}
  influx: {}
  influx-analytics: {}
  neo4j: {}
  landscaper: {}
  ringcontainer: {}
  ringcontainerexample: {}
  pkidata: {}
  vpninfo: {}
