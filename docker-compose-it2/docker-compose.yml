version: '3'
services:
####################### Proxy  ######################
  proxy:
    image: traefik
    restart: unless-stopped
    command: --web  --docker --docker.exposedByDefault=false --loglevel=info
    volumes:
     - /var/run/docker.sock:/var/run/docker.sock:ro
     - ./traefik/traefik.toml:/traefik.toml
     - ./traefik/cert:/ssl:ro
    ports:
     - "80:80"
     - "443:443"
    expose:
     - 443
    network_mode: "host"
######################################################

######################## CIMI  #######################
  cimi:
    image: mf2c/cimi-server:2.4-SNAPSHOT
    restart: on-failure
    depends_on:
      # - elasticsearch
      - logicmodule1
      - dcproxy
    environment:
      - DC_HOST=dcproxy
      - DC_PORT=6472
      - EPHEMERAL_DB_BINDING_NS=com.sixsq.slipstream.db.dataclay.loader
      - PERSISTENT_DB_BINDING_NS=com.sixsq.slipstream.db.dataclay.loader
    expose:
      - "8201"
    labels:
     - "traefik.enable=true"
     - "traefik.backend=cimi"
     - "traefik.frontend.rule=PathPrefix:/,/"
    volumes:
     - ringcontainer:/opt/slipstream/ring-container
     - ringcontainerexample:/opt/slipstream/ring-example

  rc:
    image: sixsq/ring-container:3.53-SNAPSHOT
    expose:
     - "5000"
    volumes:
     - ringcontainer:/opt/slipstream/ring-container
     - ringcontainerexample:/opt/slipstream/ring-example
    command: sh

  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2
  #   container_name: elasticsearch
  #   expose:
  #    - "9200"
  #    - "9300"
  #   environment:
  #    - cluster.name=elasticsearch
  #    - xpack.security.enabled=false
  #    - discovery.type=single-node
  #    - "ES_JAVA_OPTS=-Xms2048m -Xmx2048m"
######################################################

################## Event Service  ####################
  # eventService:
  #   image: sixsq/cimi-sse:1.0
  #   depends_on:
  #    - cimi
  #   ports:
  #    - "8000:8000"
  #   restart: unless-stopped
######################################################

#################### Dataclay  #######################
  dcproxy:
    image: mf2c/dataclay-proxy:2.4
    depends_on:
      - logicmodule1
      - ds1java1
    expose:
      - "6472"

  logicmodule1:
    image: "bscdataclay/logicmodule:mf2c"
    ports:
      - "1034:1034"
    env_file:
      - ./env/LM.environment
    environment:
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/log4j2.xml:ro

  ds1java1:
    image: "bscdataclay/dsjava:mf2c"
    depends_on:
      - logicmodule1
    env_file:
      - ./env/DS.environment
      - ./env/LM.environment
    environment:
      - DATASERVICE_NAME=DS1
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/log4j2.xml:ro
######################################################

###################### COMPSs  #######################
  COMPSs:
    image: mf2c/compss-agent:it2
    expose:
     - 8080
    restart: unless-stopped
######################################################

################## Service Manager ###################
  service-manager:
    image: mf2c/service-manager:1.6.0
    restart: unless-stopped
    depends_on:
      - cimi
    expose:
      - 46200
    ports:
      - 46200:46200
    environment:
      - JAVA_OPTS=-Xms32m -Xmx256m
######################################################

########## Lifecycle Manager + User Manager ##########
  lm-um:
    image: mf2c/lifecycle-usermngt:0.3.2
    restart: on-failure
    expose:
      - "46300"
      - "46000"
    ports:
      - "46000:46000"
      - "46300:46300"
    environment:
      - DOCKER_SWARM=True
      - K8S_MASTER=False
      - HOST_IP=192.168.252.41
      - DATACLAY_EP=
      - WORKING_DIR_VOLUME=/tmp/compose_files
    volumes:
      - /tmp/compose_files:/tmp/compose_files
      - /var/run/docker.sock:/var/run/docker.sock
######################################################

#################### SLA Manager #####################
  slalite:
    image: mf2c/sla-management:0.3.0
    restart: on-failure
    expose:
     - "46030"
    environment:
     - CIMI_URL=https://proxy:443/api
######################################################

################## Resource Manager ##################
  # policies:
  #    image: mf2c/policies:0.1.7
  #    restart: on-failure
  #    depends_on:
  #      - discovery
  #    # container_name: policies
  #    ports:
  #      - "46050"
  #      - "46051:46051"
  #      - "46052:46052"
  #
  # discovery:
  #  image: mf2c/discovery:1.0
  #  restart: on-failure
  #  depends_on:
  #    - cimi
  #  expose:
  #    - 46040
  #  cap_add:
  #    - NET_ADMIN
  #
  # identification:
  #   image: mf2c/identification:1.0
  #   restart: on-failure
  #   volumes:
  #     - mydata:/data
  #   expose:
  #     - 46060
  #
  # cau-client:
  #   image: mf2c/cau-client:v1.03
  #   depends_on:
  #     - policies
  #   expose:
  #     - 46065
  #   environment:
  #     - CAU_URL=127.0.0.1:46400
  #     - LCAU_URL=127.0.0.1:46410
  #
  # resouce-categorization:
  #   image: mf2c/resource-categorization:latest-V2.0.4
  #   restart: on-failure
  #   depends_on:
  #     - cimi
  #   expose:
  #     - 46070
  #   privileged: true
######################################################

##### Analytics Engine + Recomender + Landscaper #####
  analytics_engine:
     image: mf2c/analytics-engine:0.1
     ports:
       - 46020:46020
     expose:
       - "46020"
     depends_on:
       - influxdb
       - web
     volumes:
       - ./analytics_engine.conf:/analytics_engine/analytics_engine.conf
     network_mode: "host"
     labels:
     - "traefik.enable=true"
     - "traefik.backend=analytics_engine"
     - "traefik.frontend.rule=PathPrefix:/mf2c/optimal"
     #- "traefik.frontend.rule=Host:mf2c/optimal"
     - "traefik.port=46020"     


  landscaper:
     image: mf2c/landscaper:0.2
     volumes:
     - landscaper:/landscaper/data
     - ./landscaper.cfg:/landscaper/landscaper.cfg
     depends_on:
       - cimi
       - neo4j
       - proxy
     environment:
       - PYTHONPATH=/landscaper
       - OS_TENANT_NAME=
       - OS_PROJECT_NAME=
       - OS_TENANT_ID=
       - OS_USERNAME=
       - OS_PASSWORD=
       - OS_AUTH_URL=
     network_mode: "host"
     working_dir: /landscaper
     command: ["./start.sh", "http://localhost:7474"]

  snaptemp:
     image: mf2c/snaptemp:0.1
     command: sh /download.sh
     depends_on:
       - snap

  influxdb:
     image: influxdb
     expose:
       - "8086"
     volumes:
       - influx:/var/lib/influxdb
     environment:
       - INFLUXDB_DB=snap
       - INFLUXDB_USER=snap
       - INFLUXDB_USER_PASSWORD=snap
       - INFLUXDB_ADMIN_USER=admin
       - INFLUXDB_ADMIN_USER_PASSWORD=admin
       - INFLUXDB_HTTP_LOG_ENABLED=false
  snap:
     image: intelsdi/snap:xenial
     ports:
       - "8181:8181"
     volumes: ['/proc:/proc_host', '/sys/fs/cgroup:/sys/fs/cgroup', '/var/run/docker.sock:/var/run/docker.sock']
     depends_on:
       - influxdb
     environment:
       - SNAP_VERSION=2.0.0
       - SNAP_LOG_LEVEL=2

  neo4j:
     image: neo4j:2.3.12
     volumes:
     - neo4j:/data
     ports:
     - "7475:7474" # expose the port for the console ui
     expose:
       - "7474"
     hostname:
       neo4j
     network_mode: "host"
     environment:
     - NEO4J_AUTH=neo4j/password # neo4j requires change from default password, should reflect landscape.cfg

  web:
     image: mf2c/landscaper-api:0.1
     volumes:
       - ./landscaper.cfg:/landscaper/landscaper.cfg
     environment:
       - PYTHONPATH=/landscaper
     ports:
       - "9001:9001"
     expose:
       - "9001"
     depends_on:
       - neo4j
       - landscaper
     network_mode: "host"
     working_dir: /landscaper/landscaper/web
     command: ["/usr/local/bin/gunicorn", "-b", "0.0.0.0:9001", "-w", "2", "application:APP"]
######################################################

volumes:
  mydata: {}
  influx: {}
  influx-analytics: {}
  neo4j: {}
  landscaper: {}
  ringcontainer: {}
  ringcontainerexample: {}
